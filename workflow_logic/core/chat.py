from pydantic import BaseModel, Field
from typing import List, Union, Tuple, Optional
import uuid
from workflow_logic.util.utils import LLMConfig
from workflow_logic.util.task_utils import MessageDict, TaskResponse
from workflow_logic.core.agent import PIKAAgent
from workflow_logic.core.tasks import PIKATask
from autogen.agentchat import ConversableAgent

default_system_message = {
    "name": "pika_default",
    "content": "You are PIKA, an AI personal assistant powered by a suite of tools. Your job is to assist your user to the best of your abilities."
}

class PIKAChat(BaseModel):
    id: str = Field(default="", description="The unique ID of the chat conversation, must match the ID in the database", alias="_id")
    messages: List[MessageDict] = Field(..., description="List of messages in the chat conversation")
    pika_agent: PIKAAgent = Field(
        default = PIKAAgent(
            name="PIKA",
            system_message=default_system_message,
        ), 
        description="The PIKA agent object. Default is base PIKA Agent.")
    functions: Optional[List[PIKATask]] = Field([], description="List of functions to be registered with the agent")
    executor: PIKAAgent = Field(
        default = PIKAAgent(name="executor_agent", 
                             system_message={"name": "executor_agent", "content":"Executor Agent. Executes the code and returns the result."}, 
                             autogen_class="UserProxyAgent", 
                             code_execution_config=True, 
                             default_auto_reply=""),
        description="The executor agent object. Default is base PIKA Agent.")
    llm_config: Optional[LLMConfig] = Field(None, description="The configuration for the LLM agent")
    task_responses: List[TaskResponse] = Field([], description="List of task responses in the chat conversation")

    def generate_response(self, new_messages: Optional[List[MessageDict]] = []) -> List[MessageDict]:
        if self.functions:
            llm_agent, executor = self.register_functions(self.functions, self.executor)
            response_1 = llm_agent.generate_reply(self.messages)
            if not response_1:
                raise ValueError("No response generated by LLM agent.")
            message_1 = MessageDict(content=response_1, role="assistant", generated_by="llm", assistant_name="PIKA", type="text")
            self.messages.append(message_1) # STORE IN DB?
            new_messages.append(message_1)
            exec_response = executor.generate_reply(self.messages)
            if exec_response:
                if isinstance(exec_response, TaskResponse):
                    self.task_responses.append(exec_response)
                    message_2 = MessageDict(content=str(exec_response), role="tool", generated_by="tool", assistant_name="Task Executor", type="TaskResponse", step=exec_response.task_name)
                else:
                    message_2 = MessageDict(content=exec_response, role="tool", generated_by="tool", assistant_name="Task Executor", type="text")
                self.messages.append(message_2) # STORE IN DB?
                new_messages.append(message_2)
                return self.generate_response(new_messages=new_messages)
        else:
            response_1 = self.get_autogen_agent().generate_reply(self.messages)
            message_1 = MessageDict(content=response_1, role="assistant", generated_by="llm", assistant_name="PIKA", type="text")
            new_messages.append(message_1) # STORE IN DB?
        return new_messages # Or store them all together? I'm inclined to always store to avoid errors preventing long chains of messages from being stored

    def get_autogen_agent(self) -> ConversableAgent:
        return self.pika_agent.get_autogen_agent(llm_config=self.llm_config)
    
    def get_default_executor(self) -> ConversableAgent:
        return self.executor.get_autogen_agent()

    def register_functions(self, functions: List[PIKATask], executor: ConversableAgent, execution_history: List = []) -> Tuple[ConversableAgent, ConversableAgent]:
        for task in functions:
            task_function = task.get_function(execution_history=execution_history)
            llm_agent = self.get_autogen_agent()
            llm_agent.update_tool_signature(task_function["tool_dict"], is_remove=False)
            executor = self.get_default_executor()
            executor.register_function(task_function["function_map"])
        return llm_agent, executor